---
title: 爬虫
date: 2022-12-06 15:00:16
tags:
- 爬虫
categories:
- [前端]
---

爬虫的难点在于爬虫与反爬虫的博弈

# urllib

## 基本使用

```python
# 导入模块
import urllib.request
# 目标网址
url = 'http://www.baidu.com'
# 发送请求
response = urllib.request.urlopen(url)
# 获取源码
content = response.read()
# 解码
content = content.decode('utf-8')
print(content)
```

## 一个类型和六个方法

```python
# 类型为HTTPResponse
print(type(response.read()))
# 读取2个字节
response.read(2)
# 读取一行
response.readline()
# 读取多行
response.readlines()
# 状态码
response.getcode()
# 请求地址
response.geturl()
# 请求头
response.getheaders()
```

## 下载网页、图片和视频

```python
url = 'http://www.baidu.com'
filename = 'baidu.html'
urllib.request.urlretrieve(url, filename)
# 下载在某个文件夹中
filename = './folder/baidu.html'
```

## 定制请求对象

```python
url = 'https://www.baidu.com'
headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/107.0.0.0 Safari/537.36'
}
request = urllib.request.Request(url=url, headers=headers)
response = urllib.request.urlopen(request)
```

## 编解码

### get请求的quote方法

url中需要将汉字转化为unicode编码

```python
# 导入模块
import urllib.parse
name = urllib.parse.quote('周杰伦')
url = 'https://www.baidu.com/s?wd='
url = url + name
```

### get请求的urlencode方法

url中有多个参数需要转化为unicode编码

```python
# 导入模块
import urllib.parse
base_url = 'https://www.baidu.com/s?'
data = {
    'wd': '周杰伦',
    'sex': '男'
}
new_data = urllib.parse.urlencode(data)
url = base_url + new_data
```

### post请求百度翻译结果

```python
url = 'https://fanyi.baidu.com/sug'
data = {
    'kw': 'spider'
}
# post请求参数需要编码为字节形式
data = urllib.parse.urlencode(data).encode('utf-8')
request = urllib.request.Request(url=url, data=data, headers=headers)
response = urllib.request.urlopen(request)
content = response.read().decode('utf-8')
# 字符串转化为json对象
import json
obj = json.loads(content)
print(obj)
```

## ajax

### get请求豆瓣电影第一页推荐电影

```python
# 抓包获取api
url = ''
request = urllib.request.Request(url=url, headers=headers)
# 数据下载到本地
fp = open('douban.json', 'w', encoding='utf-8')
fp.write(content)
# 另一种写法
# with open('douban.json', 'w', encoding='utf-8') as fp:
#     fp.write(content)
```

### post请求肯德基门店地址

```python
url = ''
data = urllib.parse.urlencode(data).encode('utf-8')
request = urllib.request.Request(url=url, data=data, headers=headers)
```

## 异常

```python
import urllib.error
try:
    request = urllib.request.Request(url=url, headers=headers)
	response = urllib.request.urlopen(request)
	content = response.read().decode('utf-8')
    print(content)
except urllib.error.HTTPError:
    print('HTTPError...')
except urllib.error.URLError:
    print('URLError...')
```

## handler处理器

```python
# 获取handler对象
handler = urllib.request.HTTPHandler()
# 获取opener对象
opener = urllib.request.build_opener(handler)
# 调用open方法
response = opener.open(request)
```

## 代理

```python
proxies = {
	# 快代理
    'http': '121.230.211.142:3256'
}
handler = urllib.request.ProxyHandler(proxies=proxies)
opener = urllib.request.build_opener(handler)
response = opener.open(request)

# 代理池
proxies_pool = [
    {'http': '121.230.211.142:3256111'},
    {'http': '121.230.211.142:3256222'}
]
import random
proxies = random.choice(proxies_pool)
```

# 解析

## xpath

```python
# 1.在python安装环境的Script文件夹中安装lxml库
pip install lxml -i https://pypi.douban.com/simple
# 2.导入lxml.etree
from lxml import etree
# 3.etree.parse()解析本地文件
tree = etree.parse('xx.html')
#   etree.HTML()解析服务器响应文件
tree = etree.HTML(response.read().decode('utf-8'))
# 4.tree.xpath('xpath路径查询字符串')，返回值数据类型为列表
li_list = tree.xpath('//body/ul//li')
```

> xpath基本语法：
>
> 1. 路径查询
>
>    //，查找所有子孙节点
>
>    /，查找直接子节点
>
> 2. id/class查询
>
>    //div[@id]，包含id的div
>
>    //div[@id='aa']，id为aa的div
>
>    //div[@class='bb']，class为bb的div
>
> 3. 内容查询
>
>    //div/h1/text()，h1的内容
>
>    //div//@class，或者div的class属性值
>
> 4. 模糊查询
>
>    //div[contains(@id, "he")]，id包含he字符串的div
>
>    //div[starts-with(@id, "he")]，id以he开头的div
>
> 5. 逻辑运算
>
>    //div[@id="aa" and @class="bb"]，同时满足id为aa和class为bb的div
>
>    //div[@id="aa"] | div[@class="bb"]，id为aa的div或class为bb的div

安装chrome浏览器插件xpath helper，启动快捷键为ctrl+shift+x。

在网页中打开插件，将xpath语句输入到Query框中，可快速得到查询结果。

## JsonPath

```python
# 在python安装环境的Script文件夹中安装jsonpath库
pip install jsonpath
```

举例：本地.json文件

```json
{ "store": {
    "book": [ 
      { "category": "reference",
        "author": "Nigel Rees",
        "title": "Sayings of the Century",
        "price": 8.95
      },
      { "category": "fiction",
        "author": "Evelyn Waugh",
        "title": "Sword of Honour",
        "price": 12.99
      },
      { "category": "fiction",
        "author": "Herman Melville",
        "title": "Moby Dick",
        "isbn": "0-553-21311-3",
        "price": 8.99
      },
      { "category": "fiction",
        "author": "J. R. R. Tolkien",
        "title": "The Lord of the Rings",
        "isbn": "0-395-19395-8",
        "price": 22.99
      }
    ],
    "bicycle": {
      "color": "red",
      "price": 19.95
    }
  }
}
```

```python
# jsonpath只能解析本地文件
import json
import jsonpath
obj = json.load(open('file.json'), 'r', encoding='uft-8')
# jsonpath.jsonpath(obj, 'jsonpath语法')
# 所有书的作者
authors = jsonpath.jsonpath(obj, '$store.book[*].author')
```

> 这里有个表格，说明JSONPath语法元素和对应XPath元素的对比。
>
> ![jsonpath01](jsonpath01.jpg)
>
> 对于此json文件
>
> ![jsonpath02](jsonpath02.jpg)

## BeautifulSoup

```python
# 1.在python安装环境的Script文件夹中安装bs4
pip install bs4 -i https://pypi.douban.com/simple
# 2.导入
from bs4 import BeautifulSoup
# 3.解析本地文件
soup = BeautifulSoup(open('1.html', encoding='uft-8'), 'lxml')
#   解析服务器响应文件
soup = BeautifulSoup(response.read().decode(), 'lxml')

# bs4的基础操作：
# (1) 根据标签名查找节点，找到的是第一个符合条件的节点：
print(soup.a)
# (2) 获取标签的属性：
print(soup.a.attrs)


# bs4的常见函数：
# (1) find()
# 返回的是第一个符合条件的节点
print(soup.find('a'))
# find()还可以根据标签的属性值查找符合条件的节点
# 例如下面通过title属性值查找：
print(soup.find('a',title = 's2'))
# class属性也可以被查找，但是注意class本身是python的关键字
# 因此我们要加一个下划线：class_
print(soup.find('a',class_ = "a1"))
# (2) find_all() ：返回结果是一个列表，包含了所有目标标签(这里是a标签)
print(soup.find_all('a'))
# 如果想要多标签的数据，需要在find_all中传入列表对象，如下例：['a','span']
print(soup.find_all(['a','span']))
# 如果想要限制返回的标签数量，可以加一个limit，它的值表示查找前n个数据
print(soup.find_all('li',limit = 2)) # 查找前两个数据
# (3) select()  (推荐)
# select()返回一个列表，同时和find_all一样返回所有的目标标签
print(soup.select('a'))
# 类选择器(在前端的一种叫法)：可以通过加一个 .类名 来筛选class
print(soup.select('.s1'))
# id选择器(也是前端的一种叫法)：可以通过加一个 #id名 来筛选id
print(soup.select('#s2'))
# 属性选择器：可以通过属性存在与否、属性的具体值来筛选
print(soup.select('li[id]')) # 这表示查找所有的 li标签 中含有 id 的标签li
print(soup.select('li[id = "l2"]')) # 这表示查找所有的 li标签 中含有 id 且id的值为l2的标签li
# 层级选择器：
# a.后代选择器：一个空格，查找某个标签的后代，包括儿子、孙子标签
print(soup.select('div li'))
# b.子代选择器：一个大于号，只能查找某个标签的儿子标签，不包括孙子标签
print(soup.select('div > ul > li')) # 这里写 div > li，则没有内容返回，因为li是div的孙子标签而非子标签
# 多个标签都拿到：
# 在select()中，我们无需用列表的传参表示多个标签，直接以逗号隔开多个标签即可：
print(soup.select('a,li'))
# 获取节点具体的信息的函数
# a.获取节点的内容
# 注意要加一个索引，因为select会返回一个列表，不加索引，就无法处理成字符串
obj = soup.select('#s2')[0]
# 如果标签对象中只有内容，那么string和 get_text()都可以使用
# 但是如果标签对象中有内容也有标签，那么string就无法使用，只能用get_text()
print(obj.string)
print(obj.get_text())
# b.获取某个具体的节点(标签)的属性
# 首先获取id值是s2的标签
tag = soup.select('#s2')[0]
# 返回这个标签的名字，例如a标签，就返回一个a
print(tag.name)
# 将所有的标签属性以字典的格式返回
print(tag.attrs)
# 获取某个节点的具体某一个属性，原理是可以根据attrs的字典对象特点，
# 用字典对象的get函数，传入键，来获取某一个键值对的值
print(tag.attrs.get('title')) # 获取id值是s2的标签的title属性
```

# Selenium

## Introduction

> selenium是一个自动化测试工具、有界面浏览器,支持Firefox,Chrome等众多浏览器，在爬虫中的应用主要是用来解决JS渲染的问题。
>
> selenium能够模拟真人打开浏览器，因此可以更好的获取我们需要的数据。(有时候，使用urllib库模拟浏览器的时候，会被服务器识别，返回的数据有所缺失，因此我们的确需要selenium做爬虫)

## Install

1. http://chromedriver.storage.googleapis.com/index.html

   下载与谷歌浏览器版本号对应的驱动(没有提供64位win的驱动，64位win直接下载32位的即可），解压后将chromedriver.exe文件放在爬虫程序同级目录下。

2. 在python解释器安装位置（Script文件夹）中执行安装指令，此处安装3.4版本的selenium，高版本的语法有所不同

   ```python
   pip install selenium==3.4
   ```

## Basic Usage

```python
# 导入selenium库
from selenium import webdriver
# 之前安装浏览器工具的路径
path = 'chromedriver.exe'
# 初始化浏览器操作对象
browser = webdriver.Chrome(path)
# 访问的网页地址
url = 'https://www.baidu.com'
# 模拟真人打开浏览器并传入url
browser.get(url)
# 获取网页源码
content = browser.page_source
```

## 元素定位

```python
# (1) 根据id属性的属性值找到对象(常用）：
button = browser.find_element_by_id('su')
# (2) 根据name属性的属性值找到对象：
button = browser.find_element_by_name('wd')
# (3) 根据xpath的语句找到对象(常用）：
button = browser.find_element_by_xpath('//input[@id = "su"]')
# (4) 根据标签的名称找到对象
button = browser.find_element_by_tag_name('input')
# (5) 根据CSS选择器找到对象，相当于bs4的语法(常用）：
button = browser.find_element_by_css_selector('#su')
# (6) 根据链接元素查找对象：
button = browser.find_element_by_link_text('新闻')
```

## 元素信息

```python
# 首先，拿到页面中id值是su的input输入框元素，与变量input建立绑定关系
input = browser.find_element_by_id('su')
# (1) get_attribute()函数获取标签的指定属性的属性值
# 传参是属性的名称，例如class、id等，返回这些属性的属性值
print(input.get_attribute('class'))
# (2) tag_name函数获取元素对应的标签的名称，例如元素是input标签，返回值就是input
print(input.tag_name)
# (3) text函数获取标签的文本，文本指的是标签尖括号的内容：
# 例如：<div> xxx </div> 于是获取的结果是xxx，下例输出为空
print(input.text)
```

## 交互

```python
# (1) 文本框输入指定内容：
input.send_keys('content')
# 等待2秒
import time
time.sleep(2)
# (2) 点击按钮：
button.click()
# (3) 滑到底部：
js_bottom = 'document.documentElement.scrollTop = 100000'
browser.execute_script(js_bottom)
# (4) 回到上一页：
browser.back()
# (5) 回到下一页：
browser.forward()
# (6) 关闭浏览器：
browser.quit()
```

## Phantomjs

基于selenium的无界面浏览器，即不进行css和gui渲染，运行效率比selenium高，但已淘汰、停更，使用Chrome handless。

## Chrome handless

基于selenium的无界面浏览器，即不进行css和gui渲染，运行效率比selenium高。

```python
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
chrome_options = Options()
chrome_options.add_argument('--headless')
chrome_options.add_argument('--disable-gpu')
# path这里要改成自己的谷歌浏览器的路径：
path = r'C:\Program Files (x86)\Google\Chrome\Application\chrome.exe'
chrome_options.binary_location = path
browser = webdriver.Chrome(chrome_options = chrome_options)

url = 'https://www.baidu.com'
browser.get(url)
```

# Requests

## Introduction & Install

> 它是一个Python第三方库，处理URL资源特别方便，可以完全取代之前的urllib库，并且更加精简代码量(相较于urllib库)。

在Script文件夹中安装

```
pip install requests
```

## 基本使用

```python
import requests
url = 'http://www.xxx.com'
response = requests.get(url = url)

# (1) text属性：以字符串形式返回网页源码
print(response.text) # 由于没有设置编码格式，中文会乱码
# (2) encoding属性：设置相应的编码格式：
response.encoding = 'utf-8'
# (3) url属性：返回url地址
url = response.url
# (4) content属性：返回二进制的数据
content_binary = response.content
# (5) status_code属性：返回状态码 200是正常
status_code = response.status_code
# (6) headers属性：返回响应头
headers = response.headers
```

## get请求

```python
import requests
url = 'https://www.xxx.com'
headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/88.0.4324.104 Safari/537.36'
}
data = {
    'data': 'data'
}

# 三个参数：
# url：请求路径
# params：请求参数
# kwargs：字典
# 不需要请求对象的定制

# 参数使用params进行传递
# 参数无需编码
# 同时不需要请求对象定制
# 请求路径的？字符可以加也可以省略
response = requests.get(url=url, params=data, headers=headers)
response.encoding = 'utf-8'
content = response.text
print(content)
```

## post请求

> 如果说get请求requests库只比urllib库简单一点点的话，那么post请求绝对是requests库更加便捷，它不仅省去的请求对象的定制，而且省略了参数的编码和转码的操作，可以说非常方便，只需要和get请求一样把三个参数url、data和headers传入即可，因此post请求个人强烈推荐用requests库代替urllib库。

```python
import requests
url = 'https://www.com'
headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/88.0.4324.104 Safari/537.36'
}
data = {
    'data': 'xxx'
}
 
# 四个参数：
# url：请求路径
# data：请求参数
# json数据
# kwargs：字典
response = requests.post(url=url, data=data, headers=headers)
content = response.text
 
import json
obj = json.loads(content.encode('utf-8'))
print(obj)
```

## 代理

> 最后介绍一下requests使用代理ip的方式，它又简化了urllib库，回忆urllib库代理ip，我们需要创建handler处理器，还要定义opener对象，但requests库中，我们只需要把代理ip作为一个普通的参数，传入requests.get()/requests.post()函数即可(简直太方便了！)

```python
import requests
url = 'http://www.baidu.com/s'
headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/88.0.4324.104 Safari/537.36'
}
data = {
    'wd': 'ip'
}
proxy = {
    'http:': '218.14.108.53'
}
response = requests.get(url = url, params = data,headers = headers,proxies = proxy)
content = response.text
```

# Scrapy

## Introduction & Install

> Scrapy是适用于Python的一个快速、高层次的屏幕抓取和web抓取框架，用于抓取web站点并从页面中提取结构化的数据。Scrapy用途广泛，可以用于数据挖掘、监测和自动化测试。
>
> 简单的说，scrapy给我们提供了更加简便、高效的爬虫体验，但与此同时它的工作方式和代码与之前学习的urllib库、requests库完全不同，我们需要重新学习。

在Script文件夹中安装

```
pip install scrapy -i https://pypi.douban.com/simple
```

安装报错解决方案：https://www.bilibili.com/video/BV1Db4y1m7Ho?p=90&vd_source=f6aeb2c69f04c21417224cd8bfef034e

## 初始化项目

在终端中创建项目

```
scrapy startproject 项目名称
```

进入spiders文件夹，生成爬虫文件

```
cd 项目名称/项目名称/spiders
scrapy genspider 爬虫文件的文件名如baidu 目的网站url
```

打开settings文件，注释掉下面这句话：ROBOTSTXT_OBEY = True

打开新建的爬虫文件，注释掉parse函数下的pass，加上一句print代码，例如这里print一句'hello,scrapy'

在终端，spiders文件夹目录下，运行项目

```
scrapy crawl 爬虫文件文件名
```

## 基本使用

```python
def parse(self, response):
    # (1) response.text属性：获取的是字符串形式的数据
    print(response.text)
    # (2) response.body属性：获取的是二进制形式的数据
    print(response.body)
    # (3) response.xpath()函数可以直接使用xpath解析
    # (4) response.extract() 提取selector对象的data的属性值
    # (5) response.extract_first() 提取selector列表的第一个数据
    pass
```

## 项目结构

> 1️⃣ Spiders文件夹：这文件夹我们不陌生，因为每一次新建scrapy爬虫项目后，我们都需要终端进入Spiders文件夹，生产爬虫文件。在Spiders文件夹下，又有两个文件，一个是_init_.py文件，一个是baidu.py（baidu是前面初始化中生成的爬虫文件的文件名）。_init_.py文件是我们创建项目时默认生成的一个py文件，我们用不到这个py文件，因此我们可以忽略它，另一个tc.py文件是我们爬虫的核心文件，后续的大部分代码都会写入这个文件，因此它是至关重要的py文件。
>
> 2️⃣ _init_.py文件：它和上面提到的Spiders文件夹下的_init_.py一样，都是不被使用的py文件，无需理会。
>
> 3️⃣ items.py文件：这文件定义了数据结构，这里的数据结构与算法中的数据结构不同，它指的是爬虫目标数据的数据组成结构，例如我们需要获取目标网页的图片和图片的名称，那么此时我们的数据组成结构就定义为 图片、图片名称。后续会专门安排对scrapy框架定义数据结构的学习。
>
> 4️⃣ middleware.py文件：这py文件包含了scrapy项目的一些中间构件，例如代理、请求方式、执行等等，它对于项目来说是重要的，但对于我们爬虫基础学习来说，可以暂时不考虑更改它的内容。
>
> 5️⃣ pipelines.py文件：这是我们之前在工作原理中提到的scrapy框架中的管道文件，管道的作用是执行一些文件的下载，例如图片等，后续会安排对scrapy框架管道的学习，那时会专门研究这个py文件。
>
> 6️⃣ settings.py文件：这文件是整个scrapy项目的配置文件，里面是很多参数的设置，我们会偶尔设计到修改该文件中的部分参数，例如下一部分提到的ROBOTS协议限制，就需要进入该文件解除该限制，否则将无法实现爬取。

## 工作原理

![scrapy1](scrapy1.jpg)

> 图上表示出了scrapy框架的几个组成部分：
>
> 1️⃣ spiders
>
> spiders可以理解为代表了我们人的操作，我们在操作scrapy的时候，实际上就是以spiders的身份在操作，初始的url是我们定义的。
>
> 2️⃣ 引擎
>
> 引擎是scrapy框架的中枢，从图中可以看出，引擎与所有的其他组件交互，并且交互大多带有指令性的操作，可以理解成一个指挥官。
>
> 3️⃣ 调度器
>
> 调度器，顾名思义，它是用来做调度的，简单的说就是它会把所有请求的url放入一个队列中，每一次会从队列中取出一个url，这个过程叫做调度。
>
> 4️⃣ 下载器
>
> 下载器，是用来下载数据的，它下载的是原始数据，可以理解为网页源码，这些源数据通过引擎再次交给我们用户spiders做进一步解析处理。(下载器沟通因特网，数据来源也是因特网)
>
> 5️⃣ 管道
>
> 管道的工作是下载图片、文件，它的工作与调度器下载源数据不同，它下载的是经过解析后的具体的图片、文件等数据。
>
> 最后描述一段scrapy框架的工作过程：
>
> 首先，spiders想要在某个url对应的网页下下载图片，于是spiders向引擎递交url，之后引擎把url放入调度器排队；当这个url排到队首的时候，调度器取出这个url请求，交给引擎；这时候引擎把请求给下载器，下载器访问因特网，拿到源数据，交给引擎；引擎再把源数据给spiders。经过这一波操作，源数据被spiders拿到。之后spiders解析数据，并把里面的url和需要下载的数据分离，并一起交给引擎，引擎把url和需下载的数据分别交给调度器和管道，调度器继续重复上述操作，管道下载文件。

## scrapy shell

用于调试scrapy，需要安装ipython，在终端中输入 scrapy shell 网站url 来使用

## 解析数据

进入dang.py文件

```python
import scrapy

class DangSpider(scrapy.Spider):
    name = 'dang'
    allowed_domains = ['category.dangdang.com']
    start_urls = ['http://category.dangdang.com/cp01.01.00.00.00.00.html']

    def parse(self, response):
        list = response.xpath('//ul[@id="component_59"]/li')
        for li in list:
            src = li.xpath('.//img/@data-original').extract_first()
            if src:
                src = src
            else:
                src = li.xpath('.//img/@src').extract_first()
            name = li.xpath('.//img/@alt').extract_first()
            print('====================')
            print(src, name)
        pass
```

## 定义数据结构并使用

进入items.py文件，比如说想要下载图片、图片的名字这两项内容，并把图片存成本地的jpg或png格式，图片的名字写入一个json文件中，那么在这里，定义数据结构的代码是这样的：

```python
class PipedemoItem(scrapy.Item):
    # define the fields for your item here like:
    # name = scrapy.Field()
    # 图片
    src = scrapy.Field()
    # 名字
    name = scrapy.Field()
    pass
```

进入dang.py文件，使用数据结构

```python
# from 项目名称.items import item.py中的类名
from first_project.items import FirstProjectItem

class DangSpider(scrapy.Spider):
	...
    def parse(self, response):
        list = response.xpath('//ul[@id="component_59"]/li')
        for li in list:
			...
            # 传递给数据结构
            book = FirstProjectItem(src=src, name=name)
        pass
```

## 使用管道

进入dang.py文件，使用yield关键字将数据传递给管道

```python
class DangSpider(scrapy.Spider):
	...
    def parse(self, response):
        list = response.xpath('//ul[@id="component_59"]/li')
        for li in list:
			...
            # 传递给数据结构
            book = FirstProjectItem(src=src, name=name)
            # 每获取一个book就将book交给pipelines
            yield book
        pass
```

在setting.py中，取消注释这段代码，开启管道。

>  300，指的是该管道的优先级的大小，这个值在[1,1000]之间，值越小，优先级越高，并且当多个管道同时启用时，该值才有效。

```python
ITEM_PIPELINES = {
   'first_project.pipelines.FirstProjectPipeline': 300,
}
```

在pipelines.py文件中设置管道

```python
class FirstProjectPipeline:
    # item就是yield后面的book对象
    def process_item(self, item, spider):
        # w模式会覆盖文件内容，a模式能续写
        with open('book.json', 'a', encoding='utf=8') as fp:
            # item 是对象，而write方法必须写入一个字符串，因此需要用str转化为字符串
            fp.write(str(item))
        return item
```

优化文件写入：上述方式每传递一个对象就会打开一次文件，对文件的操作过于频繁

```python
class FirstProjectPipeline:
    # 在爬虫文件执行之前执行的方法
    def open_spider(self, sipder):
        self.fp = open('book.json', 'w', encoding='utf-8')

    def process_item(self, item, spider):
        self.fp.write(str(item))
        return item
    
    # 在爬虫文件执行完之后执行的方法
    def close_spider(self,spider):
        self.fp.close()
```

## 多个管道

在pipelines.py中定义一个管道class，用于开启多个管道，同时下载图片。

新的类，由于执行的是图片的下载，无需open和close函数，默认生成的process_item()函数足够。最后只需要在process_item()中写入具体的下载代码即可。

```python
import urllib.request
class FirstProjectPipeline2:
    def process_item(self, item, spider):
        url = 'http:' + item.get('src')
        # 需要手动创建一个books文件夹，便于下载图片的保存管理
        filename = './books/' + item.get('name') + '.jpg'
        urllib.request.urlretrieve(url=url, filename=filename)
        return item
```

在settings.py文件中进行配置

```python
ITEM_PIPELINES = {
   'first_project.pipelines.FirstProjectPipeline': 300,
    # 新增的管道
   'first_project.pipelines.FirstProjectPipeline2': 301,
}
```

## 多页下载

进入dang.py文件

```python
class DangSpider(scrapy.Spider):
    name = 'dang'
    # 如果是多页下载，必须调整allowed_domains的范围，一般只写域名
    allowed_domains = ['category.dangdang.com']
    start_urls = ['http://category.dangdang.com/cp01.01.00.00.00.00.html']

    base_url = 'http://category.dangdang.com/pg'
    page = 1

    def parse(self, response):
        list = response.xpath('//ul[@id="component_59"]/li')
        for li in list:
            ...
            yield book

        if self.page < 10:
            self.page = self.page + 1
            url = self.base_url + str(self.page) + '-cp01.01.00.00.00.00.html'
            # scrapy.Request是scrapy的get请求，url是请求地址，callback是要执行的函数
            yield scrapy.Request(url=url, callback=self.parse)
        pass
```

## 不同页面下载

```python
import scrapy
from mv.items import MvItem

class MvSpider(scrapy.Spider):
    name = 'mv'
    allowed_domains = ['www.dytt8.net']
    start_urls = ['https://www.dytt8.net/html/gndy/china/index.html']

    def parse(self, response):
        #第一页的名字和第二页的图片
        a_list = response.xpath('//div[@class="co_content8"]//td[2]//a[2]')
        for a in a_list:
            name = a.xpath('./text()').extract_first()
            href = a.xpath('./@href').extract_first()
            # 第二页的地址
            url = 'https://www.dytt8.net' + href
            # 对第二页发起访问，meta用于传递数据
            yield yield scrapy.Request(url=url, callback=self.parse_second, meta={'name':name})
	
    def parse_second(self, response):
        src = response.xpath('//div[@id="Zoom"]//img/@src').extract_first()
        name = response.meta['name']
        movie = MvItem(src=src, name=name)
        yield movie
```

## CrawlSpider

### 链接提取

不同于前面创建爬虫文件

```
scrapy genspider 爬虫文件的文件名 目的网站url
```

使用CrawlSpider的话，创建爬虫文件方式如下

```
scrapy genspider -t crawl 爬虫文件的文件名 目的网站url
```

生成的爬虫文件内容有所不同

```python
import scrapy
from scrapy.linkextractors import LinkExtractor
from scrapy.spiders import CrawlSpider, Rule
from dushu.items import DushuItem

class ReadSpider(CrawlSpider):
    name = 'read'
    allowed_domains = ['www.dushu.com']
    start_urls = ['https://www.dushu.com/book/1617_1.html']

    rules = (
        # allow为正则表达式
        # follow若为false，表示只对start_urls进行链接提取
        # follow若为true，表示链接跟进，会对提取得到的每个新链接进行链接提取，反复提取
        Rule(LinkExtractor(allow=r'/book/1617_\d+\.html'), callback='parse_item', follow=False),
    )

    def parse_item(self, response):
        item = {}
        #item['domain_id'] = response.xpath('//input[@id="sid"]/@value').get()
        #item['name'] = response.xpath('//div[@id="name"]').get()
        #item['description'] = response.xpath('//div[@id="description"]').get()
        img_list = response.xpath('//div[@class="bookslist"]//img')
        for img in img_list:
            name = img.xpath('./@alt').extract_first()
            src = img.xpath('./@data-original').extract_first()
            book = DushuItem(name=name, src=src)
            yield book
```

### 数据入库

在Script文件夹中安装pymysql

```
pip install pymysql -i https://pypi.douban.com/simple
```

Navcat创建mysql数据库pachong1，表的结构需要与待传入的数据结构相同（主键id、name、src）

在settring.py中加入配置代码

```python
DB_HOST = '127.0.0.1'
DB_PORT = 3306
DB_USER = 'root'
DB_PASSWORD = 'admin'
DB_NAME = 'pachong1'
DB_CHARSET = 'utf8'

ITEM_PIPELINES = {
   'dushu.pipelines.DushuPipeline': 300,
   'dushu.pipelines.MysqlPipeline': 301,
}
```

在pipelines.py中新建管道

```python
# 加载settings文件
from scrapy.utils.project import get_project_settings
import pymysql

class MysqlPipeline:
    def open_spider(self, spider):
        settings = get_project_settings()
        self.host = settings['DB_HOST']
        self.port = settings['DB_PORT']
        self.user = settings['DB_USER']
        self.password = settings['DB_PASSWORD']
        self.name = settings['DB_NAME']
        self.charset = settings['DB_CHARSET']

        self.connect()

    def connect(self):
        self.conn = pymysql.connect(
            host=self.host,
            port=self.port,
            user=self.user,
            password=self.password,
            db=self.name,
            charset=self.charset
        )
        self.cursor = self.conn.cursor()

    def process_item(self, item, spider):
        sql = 'insert into book(name,src) values("{}","{}")'.format(item['name'], item['src'])
        # 执行sql语句
        self.cursor.execute(sql)
        # 提交
        self.conn.commit()
        return item

    def close_spider(self, spider):
        self.cursor.close()
        self.conn.close()
```

## 调整日志打印

>  日志级别：
>
> CRITICAL: 严重错误
>
> ERROR: 一般错误
>
> WARNING: 警告
>
> INFO: 一般信息
>
> DEBUG: 调试信息
>
> 默认的日志等级是DEBUG
>
> 只要出现了DEBUG或DEBUG以上等级的日志，那么这些日志会被打印

在settings.py中，增加如下代码可以调整日志打印

1. 修改日志等级（不常用）

   ```python
   # 指定日志的级别
   LOG_LEVEL = 'WARNING'
   ```

   一般使用默认的日志等级，不会去修改

2. 修改日志打印位置

   ```python
   # 将屏幕显示的信息全部记录到文件中，屏幕不再显示，注意文件后缀一定是.log
   LOG_FILE = 'logdemo.log'
   ```

## Post请求

Post请求需要携带参数，其爬虫文件的编写与Get请求有所不同

```python
import scrapy
import json

class PostSpider(scrapy.Spider):
    name = 'post'
    allowed_domains = ['fanyi.baidu.com']

    def start_requests(self):
        url = 'https://fanyi.baidu.com/sug'
        data = {
            'kw': 'final'
        }
        yield  scrapy.FormRequest(url=url, formdata=data, callback=self.parse_second)

    def parse_second(self, response):
        content = response.text
        obj = json.loads(content, encoding='utf-8')
        print(obj)
```

